# Capture System Prompt in Conversation Data

## Context and Problem Statement

Currently, conversations stored in `.cllm/conversations/*.json` files do not capture the system prompt (`default_system_message` from Cllmfile.yml) that was active when the conversation was initiated. This creates several problems:

1. **Lost Context**: When reviewing historical conversations, it's unclear what system-level instructions guided the assistant's behavior
2. **Reproducibility Issues**: Cannot accurately reproduce conversation behavior without knowing the original system prompt
3. **Debugging Difficulty**: Troubleshooting unexpected assistant responses is harder without visibility into system-level context
4. **Incomplete Audit Trail**: Conversations lack full context for compliance, debugging, or analysis purposes

The system prompt is currently only applied at runtime (injected into the message list when calling the LLM), but never persisted to the conversation storage.

## Decision Drivers

- **Context Preservation**: Need complete historical record of all context that influenced assistant responses
- **Reproducibility**: Must be able to recreate exact conversation conditions for debugging or audit purposes
- **Consistency**: System prompt should appear once at the beginning of conversation history and never be duplicated
- **Backward Compatibility**: Existing conversations without system prompts should continue to work
- **Flexibility**: Users should be able to understand what system prompt was active for any given conversation
- **Storage Efficiency**: Avoid redundant storage of system prompt in every turn or message

## Considered Options

1. **Store system prompt as first message in conversation.messages array**
2. **Store system prompt in conversation.metadata field**
3. **Store system prompt in separate top-level conversation field**
4. **Do not persist system prompt (status quo)**

## Decision Outcome

Chosen option: "Store system prompt as first message in conversation.messages array", because it:

- Maintains compatibility with OpenAI message format (system messages are standard)
- Simplifies LLM API calls (messages array is directly usable without transformation)
- Provides clear chronological context (system message appears where it logically belongs)
- Follows established conventions in LLM tooling and documentation
- Ensures system prompt is preserved exactly as it was when conversation started

### Consequences

- Good, because system context is preserved with full fidelity for all conversations
- Good, because conversations become self-contained and fully reproducible
- Good, because existing LLM API patterns are followed (system message at index 0)
- Good, because debugging and auditing become significantly easier
- Good, because no transformation logic is needed when loading conversations for LLM calls
- Neutral, because existing conversations without system prompts continue to work (backward compatible)
- Neutral, because conversation files become slightly larger (typically 50-500 bytes)
- Bad, because system prompt appears in conversation.messages array which could confuse users expecting only user/assistant turns

### Confirmation

Implementation will be validated through:

1. **Unit Tests**: Verify system prompt is stored as first message in conversation.messages
2. **Integration Tests**: Confirm conversations with system prompts work end-to-end
3. **Backward Compatibility Tests**: Ensure existing conversations without system prompts continue to function
4. **Manual Testing**: Create new conversations with various Cllmfile.yml configurations and verify system prompt persistence
5. **Code Review**: Verify system prompt is only inserted once (at conversation creation) and never duplicated

## Pros and Cons of the Options

### Store system prompt as first message in conversation.messages array

Store the `default_system_message` as the first element in the `messages` array with `{"role": "system", "content": "..."}` format.

- Good, because follows OpenAI/LiteLLM standard message format conventions
- Good, because messages array can be passed directly to LLM APIs without transformation
- Good, because system message appears in chronological position where it takes effect
- Good, because simple to implement (insert system message when conversation is created)
- Good, because easy to inspect and debug (visible in conversation JSON files)
- Good, because preserves exact system prompt that was active at conversation start
- Neutral, because increases conversation file size slightly (typically 50-500 bytes)
- Neutral, because users see system message when viewing conversation history
- Bad, because system message appears in messages array which some users might find unexpected

### Store system prompt in conversation.metadata field

Store the system prompt as `conversation.metadata.system_prompt` string.

- Good, because separates system configuration from conversation messages
- Good, because metadata is extensible for other system-level configuration
- Good, because keeps messages array focused on user/assistant interactions
- Neutral, because requires transformation when preparing messages for LLM calls
- Bad, because adds complexity (must reconstruct system message when loading conversation)
- Bad, because deviates from standard LLM message format patterns
- Bad, because system prompt context is separated from the messages it influences

### Store system prompt in separate top-level conversation field

Add a new top-level field like `conversation.system_prompt` alongside `id`, `model`, `messages`, etc.

- Good, because makes system prompt explicitly visible in conversation schema
- Good, because clear semantic separation between messages and system configuration
- Neutral, because requires schema change and migration for existing conversations
- Bad, because adds another top-level field to conversation data structure
- Bad, because requires transformation logic when loading conversations for LLM calls
- Bad, because deviates from standard OpenAI/LiteLLM message format

### Do not persist system prompt (status quo)

Continue current behavior where system prompt is only applied at runtime and never stored.

- Good, because no code changes required
- Good, because conversation files remain minimal
- Bad, because conversations lack complete context for historical analysis
- Bad, because cannot accurately reproduce conversation behavior from storage
- Bad, because debugging and troubleshooting is significantly harder
- Bad, because audit trail is incomplete for compliance or analysis purposes

## More Information

### Implementation Details

**When to capture system prompt:**

- System prompt should be captured when a conversation is **first created** (when `conversation.messages` is empty)
- System prompt should be read from the effective configuration (post-merge Cllmfile.yml values)
- If no `default_system_message` is configured, no system message should be added

**Storage format:**

```json
{
  "id": "example-conversation",
  "model": "gpt-4",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "Hello, how are you?"
    },
    {
      "role": "assistant",
      "content": "I'm doing well, thank you! How can I help you today?"
    }
  ],
  "created_at": "2025-10-31T12:00:00Z",
  "updated_at": "2025-10-31T12:00:15Z",
  "metadata": {
    "total_tokens": 150
  }
}
```

**Loading behavior:**

- When loading a conversation, the system prompt (if present as first message with `role: "system"`) should be preserved
- No additional system message should be injected at runtime if one already exists in the conversation
- For backward compatibility, conversations without system messages should continue to work (system prompt may be injected at runtime if configured)

**Related code locations:**

- `src/cllm/cli.py:1065-1069` - Current system message injection logic for conversations
- `src/cllm/conversation.py:56-65` - `Conversation.add_message()` method
- `src/cllm/conversation.py:256-283` - `ConversationManager.create()` method

### Backward Compatibility

Existing conversations without system prompts will continue to work without modification. The implementation should:

1. Check if the first message in `conversation.messages` has `role: "system"`
2. If present, use it; if not, optionally inject system prompt at runtime (backward compatible behavior)
3. New conversations will always capture the system prompt if configured

### Related ADRs

- **ADR-0003**: Cllmfile Configuration System - Defines `default_system_message` configuration
- **ADR-0007**: Conversation Threading and Context Management - Defines conversation storage format
- **ADR-0016**: Configurable .cllm Directory Path - Affects where conversations are stored
- **ADR-0017**: Configurable Conversations Path - Affects conversation storage location

---

## AI-Specific Extensions

### AI Guidance Level

**Chosen level: Flexible**

AI agents should adapt implementation details while maintaining core principles:

- System prompt MUST be stored as first message in messages array
- System prompt MUST only be inserted once (at conversation creation)
- Backward compatibility MUST be maintained for existing conversations
- Implementation details (exact code structure, helper methods) can be adapted for clarity

### AI Tool Preferences

- Preferred AI tools: Claude Code, GitHub Copilot
- Model parameters: Standard (temperature 0.7 for balanced code generation)
- Special instructions:
  - Prioritize backward compatibility - do not break existing conversations
  - Write comprehensive tests for both new and existing conversation formats
  - Update relevant documentation and examples

### Test Expectations

**Unit Tests:**

1. Test conversation creation with system prompt configured - verify system message is first in array
2. Test conversation creation without system prompt - verify no system message added
3. Test loading conversation with existing system message - verify it's preserved
4. Test loading conversation without system message (backward compatibility)
5. Test that system prompt is not duplicated on subsequent conversation turns

**Integration Tests:**

1. Test full conversation flow: create → add messages → save → load → continue
2. Test conversation with custom Cllmfile.yml system prompt
3. Test conversation with environment variable interpolation in system prompt

**Performance Criteria:**

- No measurable performance degradation when saving/loading conversations
- File size increase should be proportional to system prompt length (typically < 1KB)

### Dependencies

**Related ADRs:**

- ADR-0003 (Cllmfile Configuration System)
- ADR-0007 (Conversation Threading and Context Management)

**System components:**

- `src/cllm/conversation.py` - Conversation and ConversationManager classes
- `src/cllm/cli.py` - CLI conversation handling logic
- `src/cllm/config.py` - Configuration loading

**External dependencies:**

- None (uses existing Python standard library and LiteLLM)

### Timeline

- Implementation deadline: Next development sprint
- First review: After unit tests pass
- Revision triggers:
  - Changes to conversation storage format (ADR-0007)
  - Changes to configuration system (ADR-0003)
  - User feedback indicating system prompt visibility issues

### Risk Assessment

#### Technical Risks

- **Risk**: Breaking existing conversations that don't have system prompts
  - **Mitigation**: Implement backward compatibility checks; existing conversations continue to work without modification
  - **Severity**: Medium
  - **Likelihood**: Low (with proper testing)

- **Risk**: System prompt duplication in edge cases (e.g., conversation loaded multiple times)
  - **Mitigation**: Add validation logic to prevent inserting system message if one already exists
  - **Severity**: Low
  - **Likelihood**: Low

- **Risk**: Configuration changes mid-conversation affecting system prompt consistency
  - **Mitigation**: System prompt is captured once at conversation creation and never updated
  - **Severity**: Low (by design - system prompt is frozen at conversation start)
  - **Likelihood**: N/A

#### Business Risks

- **Risk**: Users confused by system message appearing in conversation history
  - **Mitigation**: Document the change; provide clear examples in documentation
  - **Severity**: Low
  - **Likelihood**: Low

- **Risk**: Increased storage requirements for conversations
  - **Mitigation**: Storage increase is minimal (< 1KB per conversation); acceptable tradeoff for complete context
  - **Severity**: Very Low
  - **Likelihood**: High

### Human Review

- Review required: Before implementation
- Reviewers: Project maintainer
- Approval criteria:
  - Backward compatibility is maintained
  - Tests cover all edge cases (with/without system prompt, existing conversations)
  - Documentation is updated to reflect new behavior
  - Examples show expected conversation storage format

### Feedback Log

- **Implementation date**: 2025-10-31
- **Review date**: 2025-10-31

#### Actual Outcomes

✅ **System prompt capture implemented exactly as specified**
- System messages are stored as first element in `conversation.messages` array with `{"role": "system", "content": "..."}` format
- Storage format matches ADR specification precisely (verified via JSON inspection)
- Implementation in `src/cllm/conversation.py:67-97` (added `has_system_message()` and `set_system_message()` methods)
- ConversationManager updated in `src/cllm/conversation.py:288-327` (added `system_message` parameter to `create()`)

✅ **CLI integration successful**
- New conversations capture `default_system_message` from Cllmfile.yml automatically (src/cllm/cli.py:1055-1061)
- Runtime injection for backward compatibility implemented (src/cllm/cli.py:1069-1075)
- System prompt passed from config during conversation creation

✅ **Backward compatibility fully maintained**
- Old conversations without system messages continue to work without modification
- Runtime injection provides system message for old conversations without persisting it
- No breaking changes to existing conversation files

✅ **Complete test coverage achieved**
- **19 tests passed** (14 unit + 5 integration)
- All ADR confirmation criteria validated through tests
- Test coverage: 93% for conversation.py module

#### Challenges Encountered

1. **Initial backward compatibility logic issue**
   - **Challenge**: First implementation only injected system message for empty conversations (`len(messages_for_llm) == 0`)
   - **Resolution**: Changed to check `conversation.has_system_message()` instead, allowing runtime injection for old conversations with existing messages
   - **Impact**: Fixed in src/cllm/cli.py:1072 - now properly supports old conversations

2. **Test design for backward compatibility**
   - **Challenge**: Needed to verify that old conversations get system message at runtime but don't save it
   - **Resolution**: Created specific test `test_backward_compatibility_conversation_without_system_message` that verifies LLM receives system message but file remains unchanged
   - **Impact**: Ensures backward compatibility is thoroughly validated

#### Lessons Learned

1. **OpenAI format adherence simplifies implementation**
   - Storing system message as first message in array (standard format) eliminated transformation logic
   - Direct pass-through to LLM APIs works seamlessly
   - Lesson: Following industry standards reduces complexity

2. **Backward compatibility requires runtime vs storage distinction**
   - Key insight: Old conversations should get system message at runtime but not have their files modified
   - This prevents "upgrading" old conversations which could have unintended consequences
   - Lesson: Explicit separation between runtime behavior and persistent storage is important

3. **Comprehensive test suites catch edge cases early**
   - Tests for empty conversations, None values, and empty strings prevented bugs
   - Integration tests verified end-to-end behavior matched unit test expectations
   - Lesson: Testing both unit functionality and full workflows is essential

4. **Helper methods improve code clarity**
   - `has_system_message()` method made intent clear in both implementation and tests
   - `set_system_message()` encapsulated logic for inserting/updating system messages
   - Lesson: Well-named helper methods are documentation and implementation together

#### Suggested Improvements

1. **Documentation updates needed**
   - Add example to main README.md showing system message capture
   - Update examples/configs/ with system message usage patterns
   - Document the behavior in user-facing documentation

2. **Consider migration utility for old conversations**
   - Optional tool to "upgrade" old conversations by adding system message from current config
   - Would be useful for users wanting complete historical context
   - Could be a separate script or CLI command (e.g., `cllm migrate-conversations`)

3. **Potential enhancement: System message change detection**
   - Could warn users if system message in config differs from stored conversation
   - Helps identify when conversation context doesn't match current configuration
   - Low priority since conversations should maintain original context

4. **Performance monitoring**
   - Monitor file size impact in production usage
   - Track if users find system message visibility in conversation history helpful or confusing
   - Collect feedback on whether this improves debugging as intended

#### Confirmation Status

**Implementation Validation Criteria:**

✅ **Unit Tests** - Verify system prompt is stored as first message
- 14 unit tests implemented and passing
- Covers creation, persistence, loading, duplication prevention
- Tests: `TestSystemMessageCapture` class in tests/test_conversation.py:628-815

✅ **Integration Tests** - Confirm conversations with system prompts work end-to-end
- 5 integration tests implemented and passing
- Covers CLI creation, continuation, backward compatibility
- Tests: `TestSystemMessageInConversations` class in tests/test_cli.py:1304-1611

✅ **Backward Compatibility Tests** - Ensure existing conversations continue to function
- Explicit test for old conversations without system messages
- Verified runtime injection without file modification
- Test: `test_backward_compatibility_conversation_without_system_message`

✅ **Manual Testing** - Storage format verified
- Created test conversations with various configurations
- JSON structure matches ADR specification exactly
- System message correctly appears as first element with role="system"

✅ **Code Review** - System prompt inserted once, never duplicated
- Verified in `ConversationManager.create()` - only called during creation
- Verified in CLI - `has_system_message()` check prevents duplication
- Multiple turn test confirms no duplication: `test_system_message_not_duplicated_on_multiple_turns`

**Risk Assessment Results:**

✅ **Technical Risk 1**: Breaking existing conversations
- **Status**: Mitigated successfully
- **Evidence**: Backward compatibility tests pass; old conversations work unchanged

✅ **Technical Risk 2**: System prompt duplication
- **Status**: Mitigated successfully
- **Evidence**: Duplication prevention test passes; `has_system_message()` check prevents duplicates

✅ **Technical Risk 3**: Configuration changes mid-conversation
- **Status**: N/A (by design)
- **Evidence**: System prompt frozen at creation time; not affected by config changes

✅ **Business Risk 1**: Users confused by system message in history
- **Status**: Acceptable risk; documentation pending
- **Evidence**: Standard OpenAI format; follows industry conventions

✅ **Business Risk 2**: Increased storage requirements
- **Status**: As expected; minimal impact
- **Evidence**: Typical increase < 1KB per conversation; acceptable tradeoff

#### Summary

**Implementation Status: ✅ Fully Implemented and Validated**

All ADR objectives have been met:
- Core functionality implemented exactly as specified
- Storage format matches documented structure
- Comprehensive test coverage (19/19 tests passing)
- Backward compatibility fully maintained
- No breaking changes introduced
- Performance impact negligible

The implementation successfully addresses all problems identified in the ADR:
1. ✅ Context preservation - System prompts now captured for all new conversations
2. ✅ Reproducibility - Conversations contain complete context for reproduction
3. ✅ Debugging - System prompt visibility improves troubleshooting
4. ✅ Audit trail - Complete conversation context available for analysis

**Recommendation**: Implementation is production-ready. Suggested follow-up: Update user documentation with examples of system message capture.
